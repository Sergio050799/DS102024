{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/arima.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mundo de la previsión de series temporales mediante modelos ARIMA (media móvil integrada autorregresiva) y SARIMAX (media móvil integrada autorregresiva estacional con regresores eXógenos) en Python, el panorama actual, impulsado por los datos, la capacidad de predecir con exactitud las tendencias futuras tiene un valor incalculable. Ya se trate de predecir los movimientos del mercado de valores, los patrones meteorológicos o la planificación de las ventas de productos, comprender y utilizar estas potentes herramientas estadísticas puede proporcionarle una ventaja significativa.\n",
    "\n",
    "Este notebook le guiará a través de los modelos ARIMA y SARIMAX, dos de las metodologías más utilizadas en el análisis de series temporales. Exploraremos sus fundamentos teóricos, sus implementaciones prácticas en Python y sus aplicaciones en escenarios reales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendiendo ARIMA y SARIMAX\n",
    "Antes de sumergirnos en las implementaciones de Python, aclaremos qué son los modelos ARIMA y SARIMAX y en qué se diferencian.\n",
    "\n",
    "- **Modelo ARIMA**:\n",
    "\n",
    "    El modelo ARIMA es una mezcla de tres componentes:\n",
    "\n",
    "    1. **Autorregresivo (AR)**: Esta parte capta la relación entre el valor actual de una variable y sus valores anteriores.\n",
    "    2. **Integrada (I)**: Implica diferenciar los datos una o más veces para hacer estacionaria la serie temporal.\n",
    "    3. **Media móvil (MA)**: Este componente modela el término de error como una combinación lineal de términos de error en puntos temporales anteriores.\n",
    "\n",
    "    El modelo ARIMA se denomina ARIMA(p, d, q), donde:\n",
    "\n",
    "    - *p* es el número de observaciones retardadas en el modelo AR.\n",
    "    - *d* es el grado de diferenciación.\n",
    "    - *q* es el tamaño de la ventana de la media móvil.\n",
    "\n",
    "- Modelo SARIMAX\n",
    "\n",
    "    SARIMAX amplía ARIMA añadiendo dos aspectos clave:\n",
    "\n",
    "    1. **Estacionalidad**: Tiene en cuenta las variaciones estacionales utilizando los elementos estacionales P, D, Q y m.\n",
    "    2. **Regresores exógenos**: Son variables externas que pueden influir en la variable pronosticada.\n",
    "    \n",
    "    En SARIMAX(P, D, Q, m), los parámetros representan:\n",
    "\n",
    "    - *P, D, Q*: Componentes estacionales equivalentes a p, d, q en ARIMA.\n",
    "    - *m*: Número de periodos de cada estación.\n",
    "    \n",
    "    Cuando P, D, Q y m son cero y no se incluyen variables exógenas, SARIMAX se simplifica a un modelo ARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías Python para ARIMA y SARIMAX\n",
    "\n",
    "1. **Statsmodels**:\n",
    "\n",
    "    Es una biblioteca completa para la modelización estadística. Es particularmente fácil de usar para aquellos con experiencia en R.\n",
    "\n",
    "2. **pmdarima**:\n",
    "\n",
    "    Adapta el modelo SARIMAX de statsmodels a la API de scikit-learn, por lo que es una gran opción para los usuarios familiarizados con scikit-learn.\n",
    "\n",
    "3. **skforecast**:\n",
    "\n",
    "    Adapta el modelo statsmodels SARIMAX y está diseñado para ofrecer velocidad y simplicidad, alineándose con la API de scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ForecasterSarimax\n",
    "\n",
    "La clase ForecasterSarimax permite entrenar y validar modelos ARIMA y SARIMAX utilizando la API de skforecast. ForecasterSarimax es compatible con la implementación Sarimax de skforecast, un novedoso envoltorio para los modelos estadísticos SARIMAX que también sigue la API de sklearn. Esta implementación es muy similar a pmdarima, pero se ha racionalizado para incluir sólo los elementos esenciales para skforecast, lo que se traduce en importantes mejoras de velocidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_get_promotion_state' from 'numpy._core' (c:\\Users\\garci\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bootcamp-ds-wXHT794i-py3.12\\Lib\\site-packages\\numpy\\_core\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Librerías\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\garci\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bootcamp-ds-wXHT794i-py3.12\\Lib\\site-packages\\numpy\\__init__.py:135\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _core\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    136\u001b[0m     False_, ScalarType, True_, _get_promotion_state, _no_nep50_warning,\n\u001b[0;32m    137\u001b[0m     _set_promotion_state, \u001b[38;5;28mabs\u001b[39m, absolute, acos, acosh, add, \u001b[38;5;28mall\u001b[39m, allclose,\n\u001b[0;32m    138\u001b[0m     amax, amin, \u001b[38;5;28many\u001b[39m, arange, arccos, arccosh, arcsin, arcsinh,\n\u001b[0;32m    139\u001b[0m     arctan, arctan2, arctanh, argmax, argmin, argpartition, argsort,\n\u001b[0;32m    140\u001b[0m     argwhere, around, array, array2string, array_equal, array_equiv,\n\u001b[0;32m    141\u001b[0m     array_repr, array_str, asanyarray, asarray, ascontiguousarray,\n\u001b[0;32m    142\u001b[0m     asfortranarray, asin, asinh, atan, atanh, atan2, astype, atleast_1d,\n\u001b[0;32m    143\u001b[0m     atleast_2d, atleast_3d, base_repr, binary_repr, bitwise_and,\n\u001b[0;32m    144\u001b[0m     bitwise_count, bitwise_invert, bitwise_left_shift, bitwise_not,\n\u001b[0;32m    145\u001b[0m     bitwise_or, bitwise_right_shift, bitwise_xor, block, \u001b[38;5;28mbool\u001b[39m, bool_,\n\u001b[0;32m    146\u001b[0m     broadcast, busday_count, busday_offset, busdaycalendar, byte, bytes_,\n\u001b[0;32m    147\u001b[0m     can_cast, cbrt, cdouble, ceil, character, choose, clip, clongdouble,\n\u001b[0;32m    148\u001b[0m     complex128, complex64, complexfloating, compress, concat, concatenate,\n\u001b[0;32m    149\u001b[0m     conj, conjugate, convolve, copysign, copyto, correlate, cos, cosh,\n\u001b[0;32m    150\u001b[0m     count_nonzero, cross, csingle, cumprod, cumsum, cumulative_prod,\n\u001b[0;32m    151\u001b[0m     cumulative_sum, datetime64, datetime_as_string, datetime_data,\n\u001b[0;32m    152\u001b[0m     deg2rad, degrees, diagonal, divide, \u001b[38;5;28mdivmod\u001b[39m, dot, double, dtype, e,\n\u001b[0;32m    153\u001b[0m     einsum, einsum_path, empty, empty_like, equal, errstate, euler_gamma,\n\u001b[0;32m    154\u001b[0m     exp, exp2, expm1, fabs, finfo, flatiter, flatnonzero, flexible,\n\u001b[0;32m    155\u001b[0m     float16, float32, float64, float_power, floating, floor, floor_divide,\n\u001b[0;32m    156\u001b[0m     fmax, fmin, fmod, format_float_positional, format_float_scientific,\n\u001b[0;32m    157\u001b[0m     frexp, from_dlpack, frombuffer, fromfile, fromfunction, fromiter,\n\u001b[0;32m    158\u001b[0m     frompyfunc, fromstring, full, full_like, gcd, generic, geomspace,\n\u001b[0;32m    159\u001b[0m     get_printoptions, getbufsize, geterr, geterrcall, greater,\n\u001b[0;32m    160\u001b[0m     greater_equal, half, heaviside, hstack, hypot, identity, iinfo, iinfo,\n\u001b[0;32m    161\u001b[0m     indices, inexact, inf, inner, int16, int32, int64, int8, int_, intc,\n\u001b[0;32m    162\u001b[0m     integer, intp, invert, is_busday, isclose, isdtype, isfinite,\n\u001b[0;32m    163\u001b[0m     isfortran, isinf, isnan, isnat, isscalar, issubdtype, lcm, ldexp,\n\u001b[0;32m    164\u001b[0m     left_shift, less, less_equal, lexsort, linspace, little_endian, log,\n\u001b[0;32m    165\u001b[0m     log10, log1p, log2, logaddexp, logaddexp2, logical_and, logical_not,\n\u001b[0;32m    166\u001b[0m     logical_or, logical_xor, logspace, long, longdouble, longlong, matmul,\n\u001b[0;32m    167\u001b[0m     matrix_transpose, \u001b[38;5;28mmax\u001b[39m, maximum, may_share_memory, mean, memmap, \u001b[38;5;28mmin\u001b[39m,\n\u001b[0;32m    168\u001b[0m     min_scalar_type, minimum, mod, modf, moveaxis, multiply, nan, ndarray,\n\u001b[0;32m    169\u001b[0m     ndim, nditer, negative, nested_iters, newaxis, nextafter, nonzero,\n\u001b[0;32m    170\u001b[0m     not_equal, number, object_, ones, ones_like, outer, partition,\n\u001b[0;32m    171\u001b[0m     permute_dims, pi, positive, \u001b[38;5;28mpow\u001b[39m, power, printoptions, prod,\n\u001b[0;32m    172\u001b[0m     promote_types, ptp, put, putmask, rad2deg, radians, ravel, recarray,\n\u001b[0;32m    173\u001b[0m     reciprocal, record, remainder, repeat, require, reshape, resize,\n\u001b[0;32m    174\u001b[0m     result_type, right_shift, rint, roll, rollaxis, \u001b[38;5;28mround\u001b[39m, sctypeDict,\n\u001b[0;32m    175\u001b[0m     searchsorted, set_printoptions, setbufsize, seterr, seterrcall, shape,\n\u001b[0;32m    176\u001b[0m     shares_memory, short, sign, signbit, signedinteger, sin, single, sinh,\n\u001b[0;32m    177\u001b[0m     size, sort, spacing, sqrt, square, squeeze, stack, std,\n\u001b[0;32m    178\u001b[0m     str_, subtract, \u001b[38;5;28msum\u001b[39m, swapaxes, take, tan, tanh, tensordot,\n\u001b[0;32m    179\u001b[0m     timedelta64, trace, transpose, true_divide, trunc, typecodes, ubyte,\n\u001b[0;32m    180\u001b[0m     ufunc, uint, uint16, uint32, uint64, uint8, uintc, uintp, ulong,\n\u001b[0;32m    181\u001b[0m     ulonglong, unsignedinteger, unstack, ushort, var, vdot, vecdot, void,\n\u001b[0;32m    182\u001b[0m     vstack, where, zeros, zeros_like\n\u001b[0;32m    183\u001b[0m )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# NOTE: It's still under discussion whether these aliases \u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# should be removed.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ta \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat96\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat128\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex192\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex256\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_get_promotion_state' from 'numpy._core' (c:\\Users\\garci\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\bootcamp-ds-wXHT794i-py3.12\\Lib\\site-packages\\numpy\\_core\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# pmdarima\n",
    "from pmdarima import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# statsmodels\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# # skforecast\n",
    "from skforecast.datasets import fetch_dataset\n",
    "from skforecast.model_selection import TimeSeriesFold\n",
    "\n",
    "# from skforecast.plot import set_dark_theme\n",
    "from skforecast.sarimax import Sarimax\n",
    "from skforecast.recursive import ForecasterSarimax\n",
    "from skforecast.model_selection import backtesting_sarimax, grid_search_sarimax\n",
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Datos**\n",
    "El conjunto de datos de este documento es un resumen del consumo mensual de combustible en España."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga datos\n",
    "\n",
    "datos = fetch_dataset(name='fuel_consumption', raw=True)\n",
    "datos = datos[['Fecha', 'Gasolinas']]\n",
    "datos = datos.rename(columns={'Fecha':'date', 'Gasolinas':'litters'})\n",
    "datos['date'] = pd.to_datetime(datos['date'], format='%Y-%m-%d')\n",
    "datos = datos.set_index('date')\n",
    "datos = datos.loc[:'1990-01-01 00:00:00']\n",
    "datos = datos.asfreq('MS')\n",
    "datos = datos['litters']\n",
    "display(datos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechas Train-test\n",
    "\n",
    "fin_train = '1980-01-01 23:59:59'\n",
    "print(\n",
    "    f\"Fechas train : {datos.index.min()} --- {datos.loc[:fin_train].index.max()}  \"\n",
    "    f\"(n={len(datos.loc[:fin_train])})\"\n",
    ")\n",
    "print(\n",
    "    f\"Fechas test  : {datos.loc[fin_train:].index.min()} --- {datos.loc[:].index.max()}  \"\n",
    "    f\"(n={len(datos.loc[fin_train:])})\"\n",
    ")\n",
    "datos_train = datos.loc[:fin_train]\n",
    "datos_test  = datos.loc[fin_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "fig, ax=plt.subplots(figsize=(7, 3))\n",
    "datos_train.plot(ax=ax, label='train')\n",
    "datos_test.plot(ax=ax, label='test')\n",
    "ax.set_title('Consumo mensual combustible España')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Análisis Exploratorio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un modelo ARIMA requiere un análisis exploratorio exhaustivo. Este paso crítico sirve de brújula, guiando al analista hacia una comprensión detallada de la dinámica intrínseca de los datos. Antes de entrenar un modelo ARIMA a una serie temporal, es importante realizar un análisis exploratorio para determinar, como mínimo, lo siguiente:\n",
    "\n",
    "1. Estacionariedad: Significa que las propiedades estadísticas (media, varianza...) permanecen constantes a lo largo del tiempo, por lo que las series temporales con tendencias o estacionalidad no son estacionarias. Dado que ARIMA presupone la estacionariedad de los datos, es esencial someterlos a pruebas rigurosas, como la prueba Dickey-Fuller aumentada, para evaluar que se cumple. Si se constata la no estacionariedad, las series deben diferenciarse hasta alcanzar la estacionariedad. Este análisis ayuda a determinar el valor óptimo del parámetro  d.\n",
    "\n",
    "2. Análisis de autocorrelación: Graficar las funciones de autocorrelación y autocorrelación parcial (ACF y PACF) para identificar posibles relaciones de rezago (lags) entre los valores de la serie. Este análisis visual ayuda a determinar los términos autorregresivos (AR) y de media móvil (MA) adecuados ( p y q ) para el modelo ARIMA.\n",
    "\n",
    "3. Descomposición estacional: en los casos donde se sospecha de estacionalidad, descomponer la serie en componentes de tendencia, estacionales y residuales utilizando técnicas como las medias móviles la descomposición estacional de series temporales (STL) puede revelar patrones ocultos y ayudar a identificar la estacionalidad. Este análisis ayuda a determinar los valores óptimos de los parámetros  P, D, Q y m.\n",
    "\n",
    "Estos análisis exploratorios establecen la base para empezar a construir un modelo ARIMA efectivo que capture los patrones fundamentales y las asociaciones dentro de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Estacionariedad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varios métodos para evaluar si una serie temporal es estacionaria o no estacionaria:\n",
    "\n",
    "1. Inspección visual de la serie temporal: inspeccionando visualmente el gráfico de la serie temporal, es posible identificar la presencia de una tendencia o estacionalidad notables. Si se observan estos patrones, es probable que la serie no sea estacionaria.\n",
    "\n",
    "2. Valores estadísticos: calcular estadísticos como la media y la varianza, de varios segmentos de la serie. Si existen diferencias significativas, la serie no es estacionaria.\n",
    "\n",
    "3. Pruebas estadísticas: utilizar test estadísticos como la prueba Dickey-Fuller aumentada o la prueba Kwiatkowski-Phillips-Schmidt-Shin (KPSS).\n",
    "\n",
    "El gráfico generado en el apartado anterior muestra una clara tendencia positiva, lo que indica un aumento constante a lo largo del tiempo. En consecuencia, la media de la serie aumenta con el tiempo, lo que confirma su no estacionariedad.\n",
    "\n",
    "La diferenciación es una de las técnicas más sencillas para eliminar la tendencia de una serie temporal. Consiste en generar una nueva serie en la que cada valor se calcula como la diferencia entre el valor actual y el valor anterior, es decir, la diferencia entre valores consecutivos. Matemáticamente, la primera diferencia se calcula como:\n",
    "\n",
    "$$ΔXt=Xt−Xt−1$$\n",
    "\n",
    "Donde  $X_t$ es el valor en el tiempo $t$ y $X_t−1$ es el valor en el tiempo $t−1$. Esta es conocida como diferenciación de primer orden. Este proceso se puede repetir si es necesario hasta que se alcance la estacionariedad deseada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba de Dickey-Fuller aumentada\n",
    "\n",
    "La prueba Dickey-Fuller aumentada considera como hipótesis nula que la serie temporal tiene una raíz unitaria, una característica frecuente de las series temporales no estacionarias. Por el contrario, la hipótesis alternativa (bajo la cual se rechaza la hipótesis nula) es que la serie es estacionaria.\n",
    "\n",
    "+ Hipótesis nula ( $H_O$ ): La serie tiene una raíz unitaria, no es estacionaria.\n",
    "\n",
    "+ Hipótesis alternativa ( $H_A$ ): La serie no tiene raíz unitaria, es estacionaria.\n",
    "\n",
    "Dado que la hipótesis nula supone la presencia de una raíz unitaria, el p-value obtenido debe ser inferior a un nivel de significación determinado, a menudo fijado en 0.05, para rechazar esta hipótesis. Este resultado indica la estacionariedad de la serie. La función ``adfuller()`` de la biblioteca Statsmodels permite aplicar la prueba ADF. Su resultado incluye cuatro valores: el p-value, el valor del estadístico, el número de retardos (lags) incluidos en la prueba y los umbrales del valor crítico para tres niveles diferentes de significancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba Kwiatkowski-Phillips-Schmidt-Shin (KPSS).\n",
    "\n",
    "La prueba KPSS se utiliza para verificar si hay tendencia en la serie temporal. En otras palabras, evalúa si los datos muestran una tendencia en el tiempo que podría hacer que la serie sea no estacionaria.\n",
    "\n",
    " -Hipótesis nula (H0): La serie temporal es estacionaria alrededor de una tendencia determinista (es decir, que la serie no tiene tendencia).   \n",
    " -Hipótesis alternativa (H1): La serie tiene una tendencia significativa, lo que significa que no es estacionaria.\n",
    "\n",
    "¿Qué pasa con el p-value?\n",
    "\n",
    "Si el p-value es bajo (generalmente por debajo de 0.05), entonces rechazamos la hipótesis nula (H0) y concluimos que la serie tiene una tendencia no estacionaria.\n",
    "Si el p-value es alto, no tenemos suficiente evidencia para rechazar la hipótesis nula y concluimos que la serie es estacionaria o no muestra una tendencia fuerte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**:\n",
    "Si bien ambas pruebas se utilizan para comprobar la estacionariedad:\n",
    "\n",
    "- La prueba KPSS se centra en la presencia de tendencias. Un p-value bajo indica la no estacionariedad debida a una tendencia.\n",
    "\n",
    "- La prueba ADF se centra en la presencia de una raíz unitaria. Un p-value bajo indica que la serie temporal no tiene una raíz unitaria, lo que sugiere que podría ser estacionaria.\n",
    "\n",
    "Es habitual utilizar ambas pruebas a la vez para comprender mejor las propiedades de estacionariedad de una serie temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test estacionariedad\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "datos_diff_1 = datos_train.diff().dropna()\n",
    "datos_diff_2 = datos_diff_1.diff().dropna()\n",
    "\n",
    "print('Test estacionariedad serie original')\n",
    "print('-------------------------------------')\n",
    "adfuller_result = adfuller(datos)\n",
    "kpss_result = kpss(datos)\n",
    "print(f'ADF Statistic: {adfuller_result[0]}, p-value: {adfuller_result[1]}')\n",
    "print(f'KPSS Statistic: {kpss_result[0]}, p-value: {kpss_result[1]}')\n",
    "\n",
    "print('\\nTest estacionariedad para serie diferenciada (order=1)')\n",
    "print('--------------------------------------------------')\n",
    "adfuller_result = adfuller(datos_diff_1)\n",
    "kpss_result = kpss(datos.diff().dropna())\n",
    "print(f'ADF Statistic: {adfuller_result[0]}, p-value: {adfuller_result[1]}')\n",
    "print(f'KPSS Statistic: {kpss_result[0]}, p-value: {kpss_result[1]}')\n",
    "\n",
    "print('\\nTest estacionariedad para serie diferenciada (order=2)')\n",
    "print('--------------------------------------------------')\n",
    "adfuller_result = adfuller(datos_diff_2)\n",
    "kpss_result = kpss(datos.diff().diff().dropna())\n",
    "print(f'ADF Statistic: {adfuller_result[0]}, p-value: {adfuller_result[1]}')\n",
    "print(f'KPSS Statistic: {kpss_result[0]}, p-value: {kpss_result[1]}')\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico series\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(7, 5), sharex=True)\n",
    "datos.plot(ax=axs[0], title='Serie original')\n",
    "datos_diff_1.plot(ax=axs[1], title='Diferenciación orden 1')\n",
    "datos_diff_2.plot(ax=axs[2], title='Diferenciación orden 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El p-value obtenido tras la primera diferenciación es estadísticamente significativo acorde al umbral ampliamente reconocido y aceptado de 0.05. Por lo tanto, la selección más adecuada para el parámetro ARIMA *d* es 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Análisis de autocorrelación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico de la función de autocorrelación ( Autocorrelation Function ACF) y la función de autocorrelación parcial (Partial Autocorrelation Function (PACF)) de la serie temporal proporciona información útil sobre los posibles valores adecuados de *p* y *q*. La ACF ayuda a identificar el valor de *q* (retardos en la parte de media móvil), mientras que la PACF ayuda a identificar el valor de *p* (retardos en la parte autorregresiva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de autocorrelación (ACF)\n",
    "\n",
    "La ACF calcula la correlación entre una serie temporal y sus valores retardados (lags). En el contexto de la modelización ARIMA, una caída brusca de la ACF después de unos pocos retardos indica que los datos tienen un orden autorregresivo finito. El retardo en el que cae la ACF proporciona una estimación del valor de *q*. Si el ACF muestra un patrón sinusoidal o sinusoidal amortiguado, sugiere la presencia de estacionalidad y requiere la consideración de órdenes estacionales además de órdenes no estacionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de autocorrelación para la serie original y la serie diferenciada\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 8), sharex=True)\n",
    "plot_acf(datos, ax=axs[0], lags=50, alpha=0.05)\n",
    "axs[0].set_title('Autocorrelación serie original')\n",
    "plot_acf(datos_diff_1, ax=axs[1], lags=50, alpha=0.05)\n",
    "axs[1].set_title('Autocorrelación serie diferenciada (order=1)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Función de autocorrelación parcial (PACF)\n",
    "\n",
    "La PACF mide la correlación entre un valor retardado (lag) y el valor actual de la serie temporal, teniendo en cuenta el efecto de los retardos intermedios. En el contexto de la modelización ARIMA, si la PACF se corta bruscamente después de un determinado retardo, mientras que los valores restantes están dentro del intervalo de confianza, sugiere un modelo AR de ese orden. El desfase en el que se corta el PACF da una idea del valor de *p*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelación parcial para la serie original y la serie diferenciada\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 8), sharex=True)\n",
    "plot_pacf(datos, ax=axs[0], lags=50, alpha=0.05)\n",
    "axs[0].set_title('Autocorrelación parcial serie original')\n",
    "plot_pacf(datos_diff_1, ax=axs[1], lags=50, alpha=0.05)\n",
    "axs[1].set_title('Autoorrelación parcial serie diferenciada (order=1)');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA**:\n",
    "\n",
    "Algunas reglas generales son:\n",
    "\n",
    "- Utilizar un orden del término AR p igual al numero de *lags* que cruzan el límite de significancia en el gráfico PACF.\n",
    "\n",
    "- Utilizar un orden del término MA q igual al numero de *lags* que cruzan el límite de significancia en el gráfico ACF.\n",
    "\n",
    "- Si el ACF corta en el lag q y el PACF corta en el lag p, se recomienda empezar con un modelo ARIMA(p, d, q).\n",
    "\n",
    "- Si sólo el PACF decae después del lag p, se recomienda empezar con un modelo AR(p).\n",
    "\n",
    "- Si sólo el ACF decae después del lag q, se recomienda empezar con un modelo MA(q).\n",
    "\n",
    "Estas pautas proporcionan un punto de partida útil al seleccionar los órdenes de un modelo ARIMA y pueden ser ajustadas según las características específicas de los datos en cuestión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acorde a la función de autocorrelación, el valor óptimo para el parámetro *p* es 0. Sin embargo, se va a asignar un valor de 1 para proporcionar un componente autorregresivo al modelo. En cuanto al componente *q*, la función de autocorrelación parcial sugiere un valor de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Descomposición de series temporales**\n",
    "\n",
    "La descomposición de series temporales consiste en descomponer la serie temporal original en sus componentes fundamentales: la tendencia, la estacionalidad y los residuos. Esta descomposición puede llevarse a cabo de manera aditiva o multiplicativa. Al combinar la descomposición de las series temporales con el análisis de la ACF y la PACF, se obtiene una descripción bastante completa con la que comprender la estructura subyacente de los datos y acotar el valor los parámetros ARIMA más apropiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomposición de la serie original y la serie diferenciada\n",
    "\n",
    "res_decompose = seasonal_decompose(datos, model='additive', extrapolate_trend='freq')\n",
    "res_descompose_diff_2 = seasonal_decompose(datos_diff_1, model='additive', extrapolate_trend='freq')\n",
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9, 6), sharex=True)\n",
    "res_decompose.observed.plot(ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Serie original', fontsize=12)\n",
    "res_decompose.trend.plot(ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Tendencia', fontsize=12)\n",
    "res_decompose.seasonal.plot(ax=axs[2, 0])\n",
    "axs[2, 0].set_title('Estacionalidad', fontsize=12)\n",
    "res_decompose.resid.plot(ax=axs[3, 0])\n",
    "axs[3, 0].set_title('Residuos', fontsize=12)\n",
    "res_descompose_diff_2.observed.plot(ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Series diferenciadas (order=1)', fontsize=12)\n",
    "res_descompose_diff_2.trend.plot(ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Tendencia', fontsize=12)\n",
    "res_descompose_diff_2.seasonal.plot(ax=axs[2, 1])\n",
    "axs[2, 1].set_title('Estacionalidad', fontsize=12)\n",
    "res_descompose_diff_2.resid.plot(ax=axs[3, 1])\n",
    "axs[3, 1].set_title('Residuos', fontsize=12)\n",
    "fig.suptitle('Descomposición de la serie original vs serie diferenciada', fontsize=14)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El patrón recurrente cada 12 meses sugiere una estacionalidad anual, probablemente influenciada por factores vacacionales. \n",
    "- El gráfico de ACF respalda aún más la presencia de esta estacionalidad, ya que se observan picos significativos en los lags correspondientes a los intervalos de 12 meses, confirmando la idea de patrones recurrentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Conclusiones**\n",
    "\n",
    "Basandose en los resultados del análisis exploratorio, utilizar una combinación de diferenciación de primer orden y diferenciación estacional puede ser el enfoque más apropiado. La diferenciación de primer orden es efectiva para capturar las transiciones entre observaciones y resaltar las fluctuaciones a corto plazo. Al mismo tiempo, la diferenciación estacional, que abarca un período de 12 meses y representa el cambio de un año a otro, captura de manera efectiva los patrones cíclicos inherentes en los datos. Este enfoque nos permite lograr la estacionariedad necesaria para el proceso de modelado ARIMA subsiguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferenciaciación de orden 1 combinada con diferenciación estacional\n",
    "datos_diff_1_12 = datos_train.diff().diff(12).dropna()\n",
    "\n",
    "adfuller_result = adfuller(datos_diff_1_12, autolag='AIC')\n",
    "print(f'ADF Statistic: {adfuller_result[0]}, p-value: {adfuller_result[1]}')\n",
    "kpss_result = kpss(datos_diff_1_12)\n",
    "print(f'KPSS Statistic: {kpss_result[0]}, p-value: {kpss_result[1]}')\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Comparativa de modelos de series de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1 Statsmodels**\n",
    "\n",
    "En statsmodels, se diferencia entre el proceso de definir un modelo y entrenarlo. Este enfoque puede resultar familiar para usuarios del lenguaje de programación R, pero puede parecer algo menos convencional para aquellos acostumbrados a librerías como scikit-learn o XGBoost en el ecosistema de Python.\n",
    "\n",
    "El proceso comienza con la definición del modelo, que incluye los parámetros configurables y el conjunto de datos de entrenamiento. Cuando se invoca al método de ajuste (``fit``). En lugar de modificar el objeto modelo, como es típico en las librerías de Python, statsmodels crea un nuevo objeto __SARIMAXResults__. Este objeto no solo encapsula detalles esenciales como los residuos y los parámetros aprendidos, sino que también proporciona las herramientas necesarias para generar predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo SARIMAX con statsmodels.Sarimax\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message='Non-invertible|Non-stationary')\n",
    "modelo = SARIMAX(endog = datos_train, order = (1, 1, 1), seasonal_order = (1, 1, 1, 12))\n",
    "modelo_res = modelo.fit()\n",
    "# warnings.filterwarnings(\"default\")\n",
    "modelo_res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ahora calculamos las predicciones y el error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción\n",
    "predicciones_statsmodels = modelo_res.get_forecast(steps=len(datos_test)).predicted_mean\n",
    "predicciones_statsmodels.name = 'predicciones_statsmodels'\n",
    "predicciones_statsmodels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(datos_test, predicciones_statsmodels)\n",
    "rmse= root_mean_squared_error(datos_test, predicciones_statsmodels)\n",
    "print(f\"Statmodels: MAE: {mae} y RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.2 Pmdarima**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo SARIMAX con pdmarima.Sarimax\n",
    "modelo = ARIMA(order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "modelo.fit(y=datos_train)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "modelo.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ahora calculamos las predicciones y el error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "predicciones_pdmarima = modelo.predict(len(datos_test))\n",
    "predicciones_pdmarima.name = 'predicciones_pdmarima'\n",
    "predicciones_pdmarima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(datos_test, predicciones_pdmarima)\n",
    "rmse= root_mean_squared_error(datos_test, predicciones_pdmarima)\n",
    "print(f\"Pmdarima: MAE: {mae} y RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.3 Skforecast**\n",
    "\n",
    "La clase skforecast.Sarimax envuelve el modelo statsmodels.SARIMAX y lo adapta a la API de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA model with skforecast.Sarimax\n",
    "# ==============================================================================\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message='Non-invertible|Non-stationary')\n",
    "modelo = Sarimax(order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "modelo.fit(y=datos_train)\n",
    "modelo.summary()\n",
    "# warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ahora calculamos las predicciones y el error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# ==============================================================================\n",
    "predicciones_skforecast = modelo.predict(steps=len(datos_test))\n",
    "predicciones_skforecast.columns = ['skforecast']\n",
    "predicciones_skforecast.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_skforecast = mean_absolute_error(datos_test, predicciones_skforecast['skforecast'])\n",
    "rmse= root_mean_squared_error(datos_test, predicciones_skforecast['skforecast'])\n",
    "print(f\"SKforecast: MAE: {mae} y RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✎ *Nota*\n",
    "\n",
    "*Dado que skforecast Sarimax es un envoltorio de statsmodels SARIMAX, los resultados son los mismos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparamos los resultados:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "datos_train.plot(ax=ax, label='train')\n",
    "datos_test.plot(ax=ax, label='test')\n",
    "predicciones_statsmodels.plot(ax=ax, label='statsmodels')\n",
    "predicciones_skforecast.plot(ax=ax, label='skforecast')\n",
    "predicciones_pdmarima.plot(ax=ax, label='pmdarima')\n",
    "ax.set_title('Predicciones con modelos ARIMA')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **8. ForecasterSarimax**\n",
    "\n",
    "La clase ForecasterSarimax permite entrenar y validar modelos ARIMA y SARIMAX utilizando la API de skforecast. ForecasterSarimax es compatible con dos implementaciones ARIMA-SARIMAX:\n",
    "\n",
    "- ARIMA de pmdarima: una envoltura para statsmodels SARIMAX que sigue la API de scikit-learn.\n",
    "\n",
    "- Sarimax from skforecast: una nueva envoltura de statsmodels SARIMAX que también sigue la API de sklearn. Esta implementación es muy similar a pmdarima, pero ha sido optimizada para incluir solo los elementos esenciales para skforecast, lo que resulta en mejoras significativas de velocidad.\n",
    "\n",
    "Dado que ForecasterSarimax sigue la misma API que los otros Forecasters disponibles en la librería, es muy fácil hacer una comparación robusta del rendimiento de modelos ARIMA-SARIMAX frente a otros modelos de machine learning como Random Forest or Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.1 Entrenamiento - Predicción**\n",
    "\n",
    "El proceso de entrenamiento y predicción sigue una API similar a la de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo ARIMA con ForecasterSarimax y skforecast Sarimax\n",
    "\n",
    "forecaster = ForecasterSarimax(\n",
    "                 regressor=Sarimax(order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "             )\n",
    "forecaster.fit(y=datos_train, suppress_warnings=True)\n",
    "\n",
    "# Predicción\n",
    "predicciones = forecaster.predict(steps=len(datos_test))\n",
    "predicciones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.2 Backtesting**\n",
    "\n",
    "El siguiente ejemplo muestra el uso de backtesting para evaluar el rendimiento del modelo SARIMAX al generar predicciones para los 12 meses siguientes en un plan anual. En este contexto, se genera una previsión al final de cada mes de diciembre, prediciendo valores para los 12 meses siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos de objeto forecasterSarimax\n",
    "''' A diferencia del cross-validation estándar, que puede mezclar datos de diferentes épocas, el cross-validation en \n",
    "series temporales respeta el orden temporal de los datos'''\n",
    "\n",
    "forecaster = ForecasterSarimax(\n",
    "                 regressor=Sarimax( #\n",
    "                                order=(1, 1, 1),# Estos son los hiperparámetros del modelo (p, d, q) \n",
    "                                seasonal_order=(1, 1, 1, 12),# Estos son los hiperparámetros del modelo (P, D, Q, S)\n",
    "                                maxiter=200 # Número máximo de iteraciones que el optimizador de SARIMAX puede realizar para ajustar el modelo.\n",
    "                            )\n",
    "             )\n",
    "\n",
    "#Definimos el esquema de validación cruzada con TimeSeriesFold\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "        steps              = 12,# Porque los datos son mensuales\n",
    "        initial_train_size = len(datos_train),# Tamaño inicial de los datos de entrenamiento\n",
    "        refit              = True,#Esto indica que, al final de cada iteración del backtesting, el modelo se reentrenará con los datos más recientes.\n",
    "        fixed_train_size   = False,# Este parámetro indica que el tamaño del conjunto de entrenamiento no es fijo, sino que crece con cada iteración.\n",
    ")\n",
    "\n",
    "metrica, predicciones_sarimax_f = backtesting_sarimax(\n",
    "                            forecaster            = forecaster,# Es el modelo que se usará para hacer las predicciones.\n",
    "                            y                     = datos,\n",
    "                            cv=cv,\n",
    "                            metric                = 'mean_absolute_error',\n",
    "                            n_jobs                = \"auto\",\n",
    "                            suppress_warnings_fit = True,# Esto suprime las advertencias durante el proceso de ajuste del modelo\n",
    "                            verbose               = True,# Esto permite que se muestren detalles adicionales del proceso de backtesting mientras se ejecuta\n",
    "                            show_progress         = True #Muestra una barra de progreso mientras se ejecuta el proceso de backtesting\n",
    ")\n",
    "print(f'Metrica: {metrica}' )\n",
    "print(f' predicciones: \\n {predicciones_sarimax_f.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse= root_mean_squared_error(datos_test, predicciones_sarimax_f)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico predicciones de backtesting\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "datos.loc[fin_train:].plot(ax=ax, label='test')\n",
    "predicciones_sarimax_f.plot(ax=ax)\n",
    "ax.set_title('Predicciones de backtesting con un modelo SARIMAX')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.3 Optimización de Hiperparámetros**\n",
    "\n",
    "Cuando estamos ajustando un modelo, especialmente modelos como **ARIMA** o **SARIMAX**, tenemos varios parámetros que deben ser elegidos para que el modelo funcione correctamente. Estos parámetros se conocen como **hiperparámetros** y son **p**, **d** y **q** en el caso de ARIMA:\n",
    "\n",
    "- **p**: Orden de los términos autoregresivos (AR).\n",
    "- **d**: Número de diferenciaciones (para hacerlo estacionario).\n",
    "- **q**: Orden de los términos de la media móvil (MA).\n",
    "\n",
    "Pero, ¿cómo encontramos los mejores valores para estos hiperparámetros? **Dos enfoques comunes** pueden ayudarnos a encontrar los valores ideales: **criterios estadísticos** y **técnicas de validación**.\n",
    "\n",
    "\n",
    "\n",
    "##### a. Criterios Estadísticos (Rápido y Eficiente)\n",
    "\n",
    "Este enfoque se basa en el uso de **métricas estadísticas** como el **AIC (Criterio de Información de Akaike)** o el **BIC (Criterio de Información Bayesiano)**. Estas métricas miden cuán bien se ajusta el modelo a los datos, pero con un toque importante: **penalizan la complejidad** del modelo.\n",
    "\n",
    "- **¿Cómo funciona?**\n",
    "  - El modelo calcula la **verosimilitud** (qué tan probable es que los datos sean generados por el modelo) y luego **penaliza** los modelos más complejos.\n",
    "  - Cuanto más pequeño sea el **AIC** o el **BIC**, mejor es el modelo (en términos relativos).\n",
    "  \n",
    "- **Ventaja**: \n",
    "  - **Rápido y eficiente**, ya que no se necesita hacer predicciones sobre nuevos datos. Solo se usa el conjunto de entrenamiento para evaluar el modelo.\n",
    "  \n",
    "- **Pero ojo**: Solo nos dice si un modelo es **mejor que otro**. No garantiza que el modelo sea perfecto, por lo que aún necesitamos **validarlo**.\n",
    "\n",
    "\n",
    "##### a. Técnicas de Validación (Más Lento, Pero Más Preciso)\n",
    "\n",
    "Aquí, entran en juego las **técnicas de validación**, como el **backtesting**, que simula cómo se comportaría el modelo en el \"mundo real\". ¿Por qué es útil?\n",
    "\n",
    "- **¿Cómo funciona?**\n",
    "  - Usamos **datos históricos** para evaluar cómo el modelo predice valores futuros.\n",
    "  - Durante el backtesting, se **ajusta el modelo** a los datos de entrenamiento y luego se **evalúa su rendimiento** en datos que no se vieron durante el entrenamiento.\n",
    "\n",
    "- **Ventaja**: \n",
    "  - Este enfoque ofrece una evaluación **más robusta** y más cercana a la realidad, ya que mide cómo de bien generaliza el modelo a datos desconocidos.\n",
    "\n",
    "- **Desventaja**: \n",
    "  - **Más lento**: El modelo debe ser **reentrenado** y evaluado múltiples veces, lo que lleva más tiempo.\n",
    "\n",
    "\n",
    "##### En la Práctica:\n",
    "\n",
    "- **Paso 1: Comienza con los criterios estadísticos** (AIC, BIC). Esto te dará una buena idea de qué configuraciones de los hiperparámetros pueden ser más prometedoras.\n",
    "- **Paso 2: Luego, realiza un **backtesting** para validar el modelo** en condiciones más realistas, asegurándote de que se desempeñe bien al predecir datos futuros.\n",
    "\n",
    "Para datos grandes, ambos enfoques suelen coincidir en el mismo modelo final, pero si el conjunto de datos es pequeño o las predicciones son cruciales, los **resultados del backtesting** se vuelven más importantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.4 División de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test\n",
    "\n",
    "fin_train = '1976-01-01 23:59:59'\n",
    "fin_val = '1984-01-01 23:59:59'\n",
    "print(\n",
    "    f\"Fechas entrenamiento : {datos.index.min()} --- {datos.loc[:fin_train].index.max()}  \"\n",
    "    f\"(n={len(datos.loc[:fin_train])})\"\n",
    ")\n",
    "print(\n",
    "    f\"Fechas validacion    : {datos.loc[fin_train:].index.min()} --- {datos.loc[:fin_val].index.max()}  \"\n",
    "    f\"(n={len(datos.loc[fin_train:fin_val])})\"\n",
    ")\n",
    "print(\n",
    "    f\"Fechas test          : {datos.loc[fin_val:].index.min()} --- {datos.index.max()}  \"\n",
    "    f\"(n={len(datos.loc[fin_val:])})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "datos.loc[:fin_train].plot(ax=ax, label='entrenamiento')\n",
    "datos.loc[fin_train:fin_val].plot(ax=ax, label='validación')\n",
    "datos.loc[fin_val:].plot(ax=ax, label='test')\n",
    "ax.set_title('Consumo mensual combustible España')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La combinación de criterios estadísticos con backtesting permite optimizar los hiperparámetros rápidamente y luego validar su rendimiento real. Aunque los criterios AIC/BIC son rápidos y te ayudan a encontrar una buena configuración inicial, el backtesting te permite validar si ese conjunto de parámetros realmente funciona bien en un escenario de predicción en datos futuros.\n",
    "\n",
    "#### **8.5 Grid Search**\n",
    "\n",
    "Grid Search te ayuda a explorar diferentes combinaciones de parámetros de manera estructurada, pero siempre teniendo en cuenta la validación real con backtesting, que es más robusta y realista que sólo calcular AIC/BIC en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search basado en backtesting\n",
    "forecaster = ForecasterSarimax(\n",
    "                 regressor=Sarimax(\n",
    "                                order=(1, 1, 1), # Marcador de posición inicial sustituido en gridsearch\n",
    "                                maxiter=500\n",
    "                            )\n",
    "             )\n",
    "\n",
    "param_grid = {\n",
    "    'order': [(0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (2, 1, 1)],\n",
    "    'seasonal_order': [(0, 0, 0, 0), (0, 1, 0, 12), (1, 1, 1, 12)],\n",
    "    'trend': [None, 'n', 'c'] #(sin tendencia, tendencia lineal o constante).\n",
    "}\n",
    "cv = TimeSeriesFold(\n",
    "        steps              = 12,\n",
    "        initial_train_size = len(datos_train),\n",
    "        refit              = True,\n",
    "        fixed_train_size   = False,\n",
    "    )\n",
    "\n",
    "'''grid_search_sarimax es la función que realiza el grid search utilizando el conjunto de parámetros param_grid y \n",
    "el modelo SARIMAX. Lo que hace es entrenar y evaluar el modelo para cada combinación de los hiperparámetros definidos'''\n",
    "\n",
    "resultados_grid = grid_search_sarimax(\n",
    "                        forecaster            = forecaster,\n",
    "                        y                     = datos.loc[:fin_val],\n",
    "                        param_grid            = param_grid,\n",
    "                        cv                    = cv,\n",
    "                        metric                = 'mean_absolute_error',\n",
    "                        return_best           = False,\n",
    "                        n_jobs                = 'auto',\n",
    "                        suppress_warnings_fit = True,\n",
    "                        verbose               = False,\n",
    "                        show_progress         = True\n",
    "                   )\n",
    "resultados_grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la función auto_arima de la librería pmdarima, que es un enfoque automatizado para la selección de un modelo ARIMA o SARIMA adecuado. La función busca los mejores hiperparámetros para el modelo de series temporales, optimizando la combinación de p, d, q (para ARIMA) y los componentes estacionales P, D, Q, m (para SARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto arima: seleccion basada en AIC\n",
    "\n",
    "modelo = auto_arima(\n",
    "            y                 = datos.loc[:fin_val],\n",
    "            start_p           = 0,# Comienza a buscar valores de p desde 0\n",
    "            start_q           = 0,\n",
    "            max_p             = 3,\n",
    "            max_q             = 3,\n",
    "            seasonal          = True,# Se especifica que la serie es estacional\n",
    "            test              = 'adf',# Se utiliza el test de Dickey-Fuller aumentado para determinar la estacionalidad\n",
    "            m                 = 12, # periodicidad de la estacionalidad\n",
    "            d                 = None, # El algoritmo determinará 'd'\n",
    "            D                 = None, # El algoritmo determinará 'D'\n",
    "            trace             = True,# el algoritmo imprimirá el progreso del proceso de búsqueda\n",
    "            error_action      = 'ignore',# cualquier error durante el ajuste de un modelo será ignorado\n",
    "            suppress_warnings = True,\n",
    "            stepwise          = True # El algoritmo probará de forma inteligente diferentes combinaciones de hiperparámetros\n",
    "        )\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos los dos modelos candidatos: el seleccionado por grid_search_sarimax basado en backtesting con un error absoluto medio, y el seleccionado por auto_arima basado en el AIC, al realizar predicciones para los próximos tres años en intervalos de 12 meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones de backtesting con el mejor modelo según el grid search\n",
    "\n",
    "forecaster = ForecasterSarimax(\n",
    "                 regressor=Sarimax(order=(0, 1, 1), seasonal_order=(1, 1, 1, 12), maxiter=500),\n",
    "             )\n",
    "\n",
    "cv = TimeSeriesFold(\n",
    "        steps              = 12,\n",
    "        initial_train_size    = len(datos.loc[:fin_val]),\n",
    "        refit              = True,\n",
    "        fixed_train_size   = False,\n",
    "    )\n",
    "metrica_m1, predicciones_m1 = backtesting_sarimax(\n",
    "                                forecaster            = forecaster,\n",
    "                                y                     = datos,\n",
    "                                cv=cv,\n",
    "                                metric                = 'mean_absolute_error',\n",
    "                                n_jobs                = \"auto\",\n",
    "                                suppress_warnings_fit = True,\n",
    "                                verbose               = False,\n",
    "                                show_progress         = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones de backtesting con el mejor modelo según auto arima\n",
    "\n",
    "forecaster = ForecasterSarimax(\n",
    "                 regressor=Sarimax(order=(1, 1, 1), seasonal_order=(0, 1, 1, 12), maxiter=500),\n",
    "             )\n",
    "\n",
    "metrica_m2, predicciones_m2 = backtesting_sarimax(\n",
    "                                forecaster            = forecaster,\n",
    "                                y                     = datos,\n",
    "                                cv                    = cv,\n",
    "                                metric                = 'mean_absolute_error',\n",
    "                                n_jobs                = \"auto\",\n",
    "                                suppress_warnings_fit = True,\n",
    "                                verbose               = False,\n",
    "                                show_progress         = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación de métricas\n",
    "\n",
    "print(\"Metrica (mean absolute error) del modelo grid search:\",metrica_m1)\n",
    "\n",
    "print(\"Metric (mean_absolute_error) del modelo auto arima:\",metrica_m2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "datos.loc[fin_val:].plot(ax=ax, label='test')\n",
    "predicciones_m1 = predicciones_m1.rename(columns={'pred': 'grid search'})\n",
    "predicciones_m2 = predicciones_m2.rename(columns={'pred': 'autoarima'})\n",
    "predicciones_m1.plot(ax=ax)\n",
    "predicciones_m2.plot(ax=ax)\n",
    "ax.set_title('Predicciones de backtesting con un modelo SARIMA')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración SARIMAX identificada mediante la técnica de grid search (basada en backtesting con error medio absoluto) ofrece resultados ligeramente mejores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.6 Exogenous variables**\n",
    "\n",
    "Dentro de la biblioteca statsmodels, la implementación de ARIMA-SARIMAX ofrece una valiosa característica: la posibilidad de integrar variables exógenas como factores de previsión junto a la serie temporal primaria considerada. El único requisito para incluir una variable exógena es la necesidad de conocer el valor de la variable también durante el periodo de previsión. La adición de variables exógenas se realiza utilizando el argumento exog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **9. Predicciones con un Modelo ARIMA-SARIMAX ya entrenado**\n",
    "\n",
    "Cuando trabajamos con un modelo ARIMA (y también con SARIMAX, que es una extensión de ARIMA para manejar estacionalidad y otras variables exógenas), uno de los principales retos es cómo hacer predicciones cuando los datos futuros no siguen inmediatamente después de la última observación utilizada en el entrenamiento.\n",
    "\n",
    "En un modelo ARIMA, existe un componente clave llamado **media móvil (MA)**. Este componente utiliza los **errores de las predicciones anteriores** para realizar nuevas predicciones. Por ejemplo, para predecir el valor en el **tiempo t**, el modelo necesita saber cuál fue el **error de la predicción en t-1**. Sin embargo, si no tenemos esa predicción (porque estamos tratando de predecir en el futuro), el modelo no tiene el **error de la predicción** necesario para continuar con sus cálculos. Esto hace que, en la mayoría de los casos, sea necesario **volver a entrenar** el modelo cada vez que queremos hacer una predicción.\n",
    "\n",
    "**¿Por qué no es ideal volver a entrenar el modelo cada vez?**\n",
    "1. **Tiempo:** El proceso de entrenamiento puede ser **lento** y consumir muchos recursos, especialmente si estamos trabajando con grandes volúmenes de datos.\n",
    "2. **Recursos computacionales:** Volver a entrenar el modelo cada vez puede ser **ineficiente** si no tenemos acceso rápido o suficiente capacidad de procesamiento.\n",
    "\n",
    "### **La Solución: Alimentar al Modelo con Datos Recientes**\n",
    "\n",
    "Una solución práctica es alimentar al modelo con solo los **datos más recientes** desde el momento en que se entrenó hasta el momento de la predicción. Por ejemplo, imagina que entrenamos un modelo hace 20 días usando datos diarios de los últimos tres años. Cuando queramos hacer nuevas predicciones, solo necesitaremos **los últimos 20 días de datos**, en lugar de todos los datos históricos (que serían 365 * 3 + 20 valores). \n",
    "\n",
    "Esto **reduce el tiempo y los recursos necesarios**, pero sigue permitiendo que el modelo sea capaz de realizar predicciones precisas basadas en los últimos errores y comportamientos observados.\n",
    "\n",
    "### **Automatización con la Clase ForecasterSarimax**\n",
    "\n",
    "El proceso de integrar estos nuevos datos al modelo y hacer predicciones puede ser un poco complejo, pero la clase `ForecasterSarimax` hace todo este trabajo automáticamente. A través de su parámetro **`last_window`** en el método **`predict`**, el modelo puede tomar los datos más recientes, hacer las predicciones necesarias y ajustarse de manera eficiente sin tener que volver a entrenarse completamente.\n",
    "\n",
    "Esto significa que no tienes que preocuparte por reentrenar el modelo cada vez que quieras hacer una predicción; solo alimentas los datos más recientes y el modelo se ajusta automáticamente para hacer las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de los datos en entrenamiento y last window\n",
    "\n",
    "# Establecemos la fecha límite para los datos de entrenamiento\n",
    "fin_train = '1980-01-01 23:59:59'\n",
    "                       \n",
    "# Los datos de entrenamiento son aquellos hasta la fecha de fin_train\n",
    "print(\n",
    "    f\"Fechas entrenamiento : {datos.index.min()} --- {datos.loc[:fin_train].index.max()}  \"\n",
    "    f\"(n={len(datos.loc[:fin_train])})\"\n",
    ")\n",
    "\n",
    "# Los datos de la last window son aquellos posteriores a fin_train hasta la última fecha disponible\n",
    "print(\n",
    "    f\"Fechas Last window  : {datos.loc[fin_train:].index.min()} --- {datos.index.max()}  \"\n",
    "    f\"(n={len(datos.loc[fin_train:])})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de los datos de entrenamiento y last window\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "datos.loc[:fin_train].plot(ax=ax, label='entrenamiento')\n",
    "datos.loc[fin_train:].plot(ax=ax, label='last window')\n",
    "ax.set_title('Consumo mensual combustible España')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Forecaster se entrena utilizando datos hasta el '1980-01-01' y luego utilizará la información restante como última ventana de observaciones para generar nuevas predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# El modelo se entrena con los datos hasta la fecha fin_train y los hiperparámetros d\n",
    "# el modelo son los mismos que en el ejemplo anterior\n",
    "forecaster = ForecasterSarimax(\n",
    "                regressor = Sarimax(\n",
    "                    order          = (0, 1, 1),         # ARIMA(0,1,1): parte no estacional del modelo\n",
    "                    seasonal_order = (1, 1, 1, 12),     # SARIMA(1,1,1,12): componente estacional (12 períodos)\n",
    "                    maxiter        = 500                 # Máximo número de iteraciones para optimizar el modelo\n",
    "                )\n",
    ")\n",
    "# El modelo se ajusta (entrena) utilizando los datos hasta la fecha fin_train\n",
    "forecaster.fit(y=datos.loc[:fin_train])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la predicción utilizando la \"last window\" de datos. El modelo hace las predicciones para los siguientes 12 períodos de tiempo, usando solo los datos de la \"last window\" (los más recientes disponibles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La last window contiene los datos desde fin_train hasta la última observación\n",
    "\n",
    "predicciones = forecaster.predict(\n",
    "                  steps       = 12,\n",
    "                  last_window = datos.loc[fin_train:]\n",
    "              )\n",
    "predicciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico predicciones\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "datos.loc[:fin_train].plot(ax=ax, label='entrenamiento')\n",
    "datos.loc[fin_train:].plot(ax=ax, label='last window')\n",
    "predicciones.plot(ax=ax, label='predicciones')\n",
    "ax.set_title('Consumo mensual combustible España')\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp-ds-wXHT794i-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
